{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video-Centric Deepfake Detection ‚Äî Public Notebook\n",
        "\n",
        "**Datasets used:** FakeAVCeleb (primary), CREMA-D (audio support), and Meta‚Äôs Casual Conversations (generalization).  \n",
        "**Goal:** Train a video-primary detector with supporting audio cues.\n",
        "\n",
        "**High-level preprocessing (omitting code & paths):**\n",
        "- **Video:** sample/sync to **T=128** frames per clip, resize to **112√ó112**, normalize to **[0,1]**.\n",
        "- **Audio:** extract **MFCC (n=40)** features, pad/trim to **S=200** time-steps.\n",
        "- **Labels:** binary {0 = REAL, 1 = FAKE}.\n",
        "\n",
        "> This public notebook removes Drive mounts and raw file paths. Use your own data loader to supply tensors with the shapes below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expected Inputs for Training\n",
        "\n",
        "- `video`: **Tensor** `(B, 3, T=128, 112, 112)` ‚Äî float32, values in `[0..1]`\n",
        "- `audio_mfcc`: **Tensor** `(B, S=200, 40)` ‚Äî float32\n",
        "- `label`: **int** in `{0, 1}`  (`0=REAL`, `1=FAKE`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIG (public-safe): use relative folders\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR    = Path(\"./data\")             # put your datasets here locally (not in repo)\n",
        "WEIGHTS_DIR = Path(\"./model/weights\")    # trained weights (not included)\n",
        "OUT_DIR     = Path(\"./outputs\")\n",
        "\n",
        "for p in [DATA_DIR, WEIGHTS_DIR, OUT_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note (Public-safe):**  \n",
        "> The following cells are **dummy placeholders** to make this notebook runnable without private\n",
        "> datasets and preprocessing scripts. In the private training workflow we perform video frame\n",
        "> extraction (112√ó112, T=128), MFCC extraction (40, S=200), labeling, and folder structuring.\n",
        "> For this public notebook we provide synthetic tensors with the **same shapes**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkfDrwIPTkE1"
      },
      "source": [
        "#Preprocessing (Dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2uqI8LtLAMh",
        "outputId": "37509459-70ec-4a41-cffc-6d77f6ff9118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total video files found: 4458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4458/4458 [20:41<00:00,  3.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocessing Finished!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile                          # to read from the zip file\n",
        "import os                               # os file handling\n",
        "import io                               # to extract files from a zip without saving them to disk\n",
        "import cv2                              # OpenCV - read and resize video frames\n",
        "import numpy as np                      # handle and save numerical data like video frames and audio MFCCs\n",
        "import librosa                          # audio processing library (load audio from the video and extract MFCCs)\n",
        "import moviepy.editor as mp             # extracts audio from video\n",
        "from tqdm import tqdm                   # progress bar\n",
        "\n",
        "\n",
        "def dummy_preprocess_video(T=128, H=112, W=112):\n",
        "    # Pretend we extracted and normalized frames\n",
        "    return torch.rand(3, T, H, W)  # (C=3, T, H, W) in [0,1]\n",
        "\n",
        "def dummy_preprocess_audio(S=200, n_mfcc=40):\n",
        "    # Pretend we computed MFCCs\n",
        "    return torch.rand(S, n_mfcc)   # (S, 40)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Preprocessing Finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6uFbjl6g4QM"
      },
      "source": [
        "#Renaming video files (Dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dummy_rename_files():\n",
        "    # Public placeholder: real file renaming happens in the private pipeline.\n",
        "    # Kept here only to show the step exists.\n",
        "    return \"Renaming step skipped in public notebook.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMbxMkGsmHN0"
      },
      "source": [
        "#Preprocessing, Labeling and Data Loader (Dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DummyAVDataset(Dataset):\n",
        "    def __init__(self, n=100, T=128, H=112, W=112, S=200, n_mfcc=40):\n",
        "        self.n, self.T, self.H, self.W, self.S, self.n_mfcc = n, T, H, W, S, n_mfcc\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = dummy_preprocess_video(self.T, self.H, self.W)      # (3,T,H,W)\n",
        "        audio = dummy_preprocess_audio(self.S, self.n_mfcc)         # (S,40)\n",
        "        label = torch.randint(0, 2, (1,)).item()                    # 0 or 1\n",
        "        return video, audio, label\n",
        "\n",
        "def make_dummy_loader(bs=4):\n",
        "    ds = DummyAVDataset()\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_DUMMY = True\n",
        "if USE_DUMMY:\n",
        "    train_loader = make_dummy_loader(bs=4)\n",
        "else:\n",
        "    # train_loader = YourRealPrivateLoader(...)\n",
        "    raise NotImplementedError(\"Use your private loader in non-public environment.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJtTOMvCncWB"
      },
      "source": [
        "#Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqwJUjtHmLtx",
        "outputId": "b95b102e-1265-4591-c2ba-284e5c6eae63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Improved Model ready!\n"
          ]
        }
      ],
      "source": [
        "import torch                # PyTorch library - handles tensors, models, training\n",
        "import torch.nn as nn       # contains neural network\n",
        "import torchvision          # gives access to prebuilt models => R(2+1)D\n",
        "\n",
        "# Improved Video Encoder ‚Üí r2plus1d_18 (better than r3d_18)\n",
        "class VideoEncoder(nn.Module):                                                # class for video input (nn.Module is one of PyTorch's models)\n",
        "    def __init__(self, out_features=512):                            # takes 128 frames video and then learn everything then turn it into a single 512-length feature vector\n",
        "        super(VideoEncoder, self).__init__()                         # calling the parent class (which is nn.Module to => VideoEncoder)\n",
        "        self.model = torchvision.models.video.r2plus1d_18(pretrained=False)   # uses R(2+1)D-18 deep learning model from torchvision & learns both spatial & temporal features. pretrained=false means you train them from the scratch\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, out_features)    # controlling the final layer to get the output i want\n",
        "\n",
        "    def forward(self, x):           # when a video clip is given, it passes through the R(2+1)D model and return the output\n",
        "        return self.model(x)\n",
        "\n",
        "# Audio Encoder ‚Üí same LSTM\n",
        "class AudioEncoder(nn.Module):      # class for audio input (nn.Module is one of PyTorch's models)\n",
        "    def __init__(self, input_size=40, hidden_size=128, num_layers=2):  # init runs when you create an AudioEncoder object | extract 40 MFCCs, 128 features to learn-LSTM, 2 LSTM layers\n",
        "        super(AudioEncoder, self).__init__()    # calling the parent class (which is nn.Module to => AudioEncoder)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)    # MFCC values-40, size-128 frames, layers-2, batch_first- how inputs should be in order (init objects order) **this line builds LSTM, but doesn't run it yet**\n",
        "        self.output_size = hidden_size    # stores the output size (128)\n",
        "\n",
        "    def forward(self, x):                   # this defines how audio input is processed when passed through the model\n",
        "        x = x.permute(0, 2, 1)              # [batch, time_steps, n_mfcc] re-arranging according as LSTM expects **(here originally it was [0, 1, 2] but we rearrange it and feed it into the model like a batch size of one audio clip and how many time steps and mfcc features are there)\n",
        "        output, (hn, cn) = self.lstm(x)     # then getting an output\n",
        "        return hn[-1]                       # gives you the LSTM second layer output\n",
        "\n",
        "# Final Model ‚Üí Combine Video + Audio (concat fusion for now)\n",
        "class DeepfakeDetector(nn.Module):  # creating my own model and inheriting from nn.Module all the core neural network features from PyTorch to build and train\n",
        "    def __init__(self, video_feature_size=512, audio_feature_size=128, num_classes=2):  # building the system to predict the result once its trained (output of VideoEncoder+AudioEncoder, two binary classification which is real/fake)\n",
        "        super(DeepfakeDetector, self).__init__()    # calling the parent class (which is nn.Module to => DeepfakeDetector)\n",
        "        self.video_encoder = VideoEncoder(out_features=video_feature_size)    # this creates the video encoder part\n",
        "        self.audio_encoder = AudioEncoder(input_size=40, hidden_size=audio_feature_size)    # this creates the audio encoder part\n",
        "\n",
        "        fusion_size = video_feature_size + audio_feature_size   # calculates the total size of combined feature size (512 + 128 = 640)\n",
        "#classifier\n",
        "        self.classifier = nn.Sequential(        # final decision-making block\n",
        "            nn.Linear(fusion_size, 256),        # input=640(video+audio) | output=256smaller features (reduce size while only keeping important info.)\n",
        "            nn.ReLU(),                          # helps the model learn complex patterns\n",
        "            nn.Dropout(0.3),                    # prevents overfitting\n",
        "            nn.Linear(256, num_classes)         # final layer, output => real, fake\n",
        "        )\n",
        "\n",
        "    def forward(self, video, audio):              # this function defines what happens if you input something into the model\n",
        "        video_feat = self.video_encoder(video)    # sends the video input through the VideoEncoder\n",
        "        audio_feat = self.audio_encoder(audio)    # sends the audio input through the AudioEncoder\n",
        "\n",
        "        fused = torch.cat([video_feat, audio_feat], dim=1)    # concatenates video and audio features\n",
        "        out = self.classifier(fused)                          # sends the fused feature into your classifier (the final decision-maker) - outputs 2 scores per sample\n",
        "        return out                                            # returns the final prediction to use later for loss and accuracy during training\n",
        "\n",
        "# ‚úÖ Instantiate Improved Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # checks if the system has GPU, if yes => cuda, no => cpu\n",
        "model = DeepfakeDetector().to(device)       # create the model and then move it to the GPU\n",
        "\n",
        "print(\"‚úÖ Improved Model ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwK5DMMuk66T"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8wdYbpRWet4H",
        "outputId": "9de76d4c-a18b-488b-b3a7-f667f12e73b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/100\n",
            "Train Loss: 0.3632 | Train Acc: 86.41%\n",
            "Val Loss: 0.3693 | Val Acc: 85.53%\n",
            "‚úÖ Best model saved!\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 2/100\n",
            "Train Loss: 0.2989 | Train Acc: 89.03%\n",
            "Val Loss: 0.3703 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 3/100\n",
            "Train Loss: 0.3117 | Train Acc: 88.78%\n",
            "Val Loss: 0.3645 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 4/100\n",
            "Train Loss: 0.2919 | Train Acc: 89.00%\n",
            "Val Loss: 0.3626 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 5/100\n",
            "Train Loss: 0.2854 | Train Acc: 89.09%\n",
            "Val Loss: 0.3664 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 6/100\n",
            "Train Loss: 0.2968 | Train Acc: 88.78%\n",
            "Val Loss: 0.3685 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 7/100\n",
            "Train Loss: 0.2799 | Train Acc: 89.09%\n",
            "Val Loss: 0.3632 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 8/100\n",
            "Train Loss: 0.2665 | Train Acc: 89.18%\n",
            "Val Loss: 0.3786 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 9/100\n",
            "Train Loss: 0.2672 | Train Acc: 89.12%\n",
            "Val Loss: 0.3239 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 10/100\n",
            "Train Loss: 0.2501 | Train Acc: 89.15%\n",
            "Val Loss: 0.3250 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 11/100\n",
            "Train Loss: 0.2703 | Train Acc: 88.69%\n",
            "Val Loss: 0.3441 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 12/100\n",
            "Train Loss: 0.2502 | Train Acc: 89.15%\n",
            "Val Loss: 0.3195 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 13/100\n",
            "Train Loss: 0.2506 | Train Acc: 89.28%\n",
            "Val Loss: 0.3487 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 14/100\n",
            "Train Loss: 0.2485 | Train Acc: 89.06%\n",
            "Val Loss: 0.3431 | Val Acc: 85.03%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 15/100\n",
            "Train Loss: 0.2594 | Train Acc: 89.03%\n",
            "Val Loss: 0.3456 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 16/100\n",
            "Train Loss: 0.2507 | Train Acc: 89.09%\n",
            "Val Loss: 0.3279 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 17/100\n",
            "Train Loss: 0.2433 | Train Acc: 88.88%\n",
            "Val Loss: 0.3272 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 18/100\n",
            "Train Loss: 0.2351 | Train Acc: 89.18%\n",
            "Val Loss: 0.3475 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 19/100\n",
            "Train Loss: 0.2347 | Train Acc: 89.03%\n",
            "Val Loss: 0.3222 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 20/100\n",
            "Train Loss: 0.2396 | Train Acc: 89.18%\n",
            "Val Loss: 0.3432 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 21/100\n",
            "Train Loss: 0.2460 | Train Acc: 89.15%\n",
            "Val Loss: 0.3148 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 22/100\n",
            "Train Loss: 0.2352 | Train Acc: 88.97%\n",
            "Val Loss: 0.3212 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 23/100\n",
            "Train Loss: 0.2244 | Train Acc: 89.12%\n",
            "Val Loss: 0.3180 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 24/100\n",
            "Train Loss: 0.2328 | Train Acc: 89.24%\n",
            "Val Loss: 0.5169 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 25/100\n",
            "Train Loss: 0.2511 | Train Acc: 89.09%\n",
            "Val Loss: 0.3317 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 26/100\n",
            "Train Loss: 0.2374 | Train Acc: 89.18%\n",
            "Val Loss: 0.3296 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 27/100\n",
            "Train Loss: 0.2545 | Train Acc: 88.97%\n",
            "Val Loss: 0.3309 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 28/100\n",
            "Train Loss: 0.2271 | Train Acc: 89.03%\n",
            "Val Loss: 0.3104 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 29/100\n",
            "Train Loss: 0.2419 | Train Acc: 89.12%\n",
            "Val Loss: 0.3726 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 30/100\n",
            "Train Loss: 0.2399 | Train Acc: 89.21%\n",
            "Val Loss: 0.3355 | Val Acc: 85.69%\n",
            "‚úÖ Best model saved!\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 31/100\n",
            "Train Loss: 0.2420 | Train Acc: 89.34%\n",
            "Val Loss: 0.3587 | Val Acc: 85.53%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 32/100\n",
            "Train Loss: 0.2649 | Train Acc: 89.31%\n",
            "Val Loss: 0.3535 | Val Acc: 85.69%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 33/100\n",
            "Train Loss: 0.2417 | Train Acc: 89.37%\n",
            "Val Loss: 0.3223 | Val Acc: 84.70%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 34/100\n",
            "Train Loss: 0.2273 | Train Acc: 89.15%\n",
            "Val Loss: 0.3102 | Val Acc: 86.02%\n",
            "‚úÖ Best model saved!\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 35/100\n",
            "Train Loss: 0.2262 | Train Acc: 89.03%\n",
            "Val Loss: 0.3305 | Val Acc: 85.69%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 36/100\n",
            "Train Loss: 0.2138 | Train Acc: 89.37%\n",
            "Val Loss: 0.3233 | Val Acc: 85.03%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 37/100\n",
            "Train Loss: 0.2123 | Train Acc: 89.52%\n",
            "Val Loss: 0.3275 | Val Acc: 86.02%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 38/100\n",
            "Train Loss: 0.2246 | Train Acc: 89.52%\n",
            "Val Loss: 0.3579 | Val Acc: 84.87%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 39/100\n",
            "Train Loss: 0.2468 | Train Acc: 89.55%\n",
            "Val Loss: 0.3173 | Val Acc: 84.05%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 40/100\n",
            "Train Loss: 0.2214 | Train Acc: 89.55%\n",
            "Val Loss: 0.3116 | Val Acc: 86.51%\n",
            "‚úÖ Best model saved!\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 41/100\n",
            "Train Loss: 0.2209 | Train Acc: 89.43%\n",
            "Val Loss: 0.2914 | Val Acc: 85.36%\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 42/100\n",
            "Train Loss: 0.1841 | Train Acc: 92.05%\n",
            "Val Loss: 0.2568 | Val Acc: 89.14%\n",
            "‚úÖ Best model saved!\n",
            "üíæ Checkpoint saved for this epoch!\n",
            "\n",
            "Epoch 43/100\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ff158827d405>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch                        # PyTorch library - handles tensors, models, training\n",
        "import torch.nn as nn               # contains neural network\n",
        "import torch.optim as optim         # optimization algorithms like Adam\n",
        "import os                           # for handling folder and file paths\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define Loss (classification) and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # this tells the model how wrong its predictions are (the loss function compares model output to the true label)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)         # this is optimizer - Adam, learning rate is small => smaller steps for stability\n",
        "\n",
        "# Save path in Google Drive\n",
        "save_path = \"path to save the checkpoints\"   # save the model checkpoints\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Training settings\n",
        "num_epochs = 100\n",
        "best_val_acc = 0.0      # best validation accuracy so far\n",
        "\n",
        "for epoch in range(num_epochs):               # repeats training for 100 epochs\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Training phase\n",
        "    # ------------------------------\n",
        "    model.train()         # sets the model to training mode\n",
        "    train_loss = 0\n",
        "    correct = 0           # these variables help track total loss and accuracy during training\n",
        "    total = 0\n",
        "\n",
        "    for video, audio, labels in train_loader:   # loop through training data in batches\n",
        "        video = video.to(device)\n",
        "        audio = audio.to(device)                # move data to GPU or CPU, depending on device\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()           # clears old gradients so they don‚Äôt mix with the new ones\n",
        "\n",
        "        outputs = model(video, audio)       # get predictions from the model\n",
        "        loss = criterion(outputs, labels)   # compare predictions with true labels using loss function\n",
        "\n",
        "        loss.backward()     # compute how the weights should change based on the loss\n",
        "        optimizer.step()    # apply the changes (update the weights to reduce loss)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)                   # update the training loss and count how many predictions were correct\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100. * correct / total\n",
        "    print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%\")    # calculate average loss and accuracy for this epoch\n",
        "\n",
        "    # ------------------------------\n",
        "    # Validation phase\n",
        "    # ------------------------------\n",
        "    model.eval()      # sets model to evaluation model\n",
        "    val_loss = 0\n",
        "    correct = 0       # these variables help track total loss and accuracy during evaluation\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():   # don't use gradient (gradient tells your model how much to change each weight to make predictions better)\n",
        "        for video, audio, labels in val_loader:   # loop through training data in batches\n",
        "            video = video.to(device)\n",
        "            audio = audio.to(device)              # move data to GPU or CPU, depending on device\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(video, audio)         # get predictions from the model\n",
        "            loss = criterion(outputs, labels)     # compare predictions with true labels using loss function\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)                 # update the training loss and count how many predictions were correct\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100. * correct / total\n",
        "    print(f\"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_acc:.2f}%\")      # show how well the model did on the validation data\n",
        "\n",
        "    # ------------------------------\n",
        "    # Save best model\n",
        "    # ------------------------------\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        save_file = os.path.join(save_path, \"model.pth\")   # if this epoch had the highest accuracy so far, save the model as best_model.pth\n",
        "        torch.save(model.state_dict(), save_file)\n",
        "        print(\"‚úÖ Best model saved!\")\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    checkpoint_file = os.path.join(save_path, f\"epoch_{epoch+1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_file)\n",
        "    print(\"üíæ Checkpoint saved for this epoch!\")                        # save the model at the end of every epoch\n",
        "\n",
        "print(\"\\nüéâ Training Finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training was continued for **290** epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxafKW7y-QKa"
      },
      "source": [
        "#Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_9nKGWOjV0K",
        "outputId": "8ee111cd-88bf-4d3f-8020-76d8cd030a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Test Accuracy: 96.69%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        REAL       0.97      0.98      0.97       200\n",
            "        FAKE       0.97      0.95      0.96       132\n",
            "\n",
            "    accuracy                           0.97       332\n",
            "   macro avg       0.97      0.96      0.97       332\n",
            "weighted avg       0.97      0.97      0.97       332\n",
            "\n",
            "\n",
            "‚úÖ AUC-ROC Score: 0.9909\n"
          ]
        }
      ],
      "source": [
        "import torch        # for loading and running PyTorch model\n",
        "import numpy as np  # for handling arrays\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score      # for calculating evaluation metrics\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # checks if GPU is available if not use CPU\n",
        "\n",
        "# ‚úÖ Load best fine-tuned model\n",
        "model.load_state_dict(torch.load(\"model path\")) # loads the saved model weights\n",
        "model.to(device)    # moves it to the selected device\n",
        "model.eval()        # puts the model in evaluation mode\n",
        "\n",
        "# ‚úÖ Evaluate\n",
        "all_labels, all_preds, all_probs = [], [], []   # all_labels => 0/1 | all_preds => what the model predicted (0 or 1) | all_probs => confidence score for AUC\n",
        "\n",
        "with torch.no_grad():   # don't calculate gradient (since we are not training anymore)\n",
        "    for video, audio, labels in test_loader:    # batch of video features | batch of audio features | labels (real or fake)\n",
        "        video = video.to(device)\n",
        "        audio = audio.to(device)      # move data to GPU/CPU\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(video, audio)                 # run the model\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]   # converts the probabilities (softmax => turns raw scores into probabilities) | [:, 1] picks the probability of the FAKE class\n",
        "        _, predicted = outputs.max(1)     # finds the index of the higher score (So it picks either REAL (0) or FAKE (1) as the prediction)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())  # save everything, converts to numpy arrays, adds them to the result lists for final metric calculations\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "print(f\"\\n‚úÖ Test Accuracy: {accuracy_score(all_labels, all_preds) * 100:.2f}%\")  # calculate and print the metrics (measures the percentage of correct predictions out of all predictions)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"REAL\", \"FAKE\"]))\n",
        "print(f\"\\n‚úÖ AUC-ROC Score: {roc_auc_score(all_labels, all_probs):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrBVbc4QrCc"
      },
      "source": [
        "#Graph (Dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Dummy predictions and labels\n",
        "y_true = np.array([0, 0, 1, 1, 0, 1])\n",
        "y_pred = np.array([0, 0, 1, 0, 0, 1])\n",
        "y_score = np.array([0.1, 0.4, 0.9, 0.6, 0.2, 0.8])\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhR-LdgQvkq"
      },
      "source": [
        "#Saving the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "save_path = Path(\"model/weights/model.pth\")\n",
        "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# torch.save(model.state_dict(), save_path)  # ‚Üê leave commented in public notebook\n",
        "print(\"Weights would be saved to:\", save_path.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbIB2dDLQ4GC"
      },
      "source": [
        "#Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Inference:** See `model/inference.py` for how to run predictions if weights are available.  \n",
        "Pretrained weights are not included in this repository.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
